name: ðŸ§© EconoDeal â€“ Canadian Tire Scraper (503 stores)

on:
  schedule:
    - cron: "0 1 * * *"   # 01:00 UTC â‰ˆ 21:00 QC
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Show repository structure (debug)
        run: |
          echo "ðŸ“‚ Repo tree (top 200 lines):"
          pwd
          ls -R | head -n 200

      # ðŸ”§ GÃ©nÃ¨re data/canadiantire/stores_ids.txt s'il n'existe pas,
      # Ã  partir de data/canadiantire/stores.json OU data/canadian-tire/stores.json
      - name: Generate stores_ids.txt if missing
        run: |
          set -e
          python - <<'PY'
          import json, re, pathlib, sys
          out = pathlib.Path('data/canadiantire/stores_ids.txt')
          if out.exists():
              print(f"[OK] stores_ids.txt already exists ({out})")
              sys.exit(0)

          # Cherche une source de magasins (avec ou sans tiret)
          sources = [
              pathlib.Path('data/canadiantire/stores.json'),
              pathlib.Path('data/canadian-tire/stores.json'),
              pathlib.Path('data/canadiantire/magasin.json'),
              pathlib.Path('data/canadian-tire/magasin.json'),
          ]
          src = next((p for p in sources if p.exists()), None)
          if not src:
              print("[ERROR] Aucune source (stores.json / magasin.json) trouvÃ©e.", file=sys.stderr)
              sys.exit(1)

          raw = src.read_text(encoding='utf-8', errors='ignore')
          ids = set()

          # Essaye JSON structurÃ©
          try:
              obj = json.loads(raw)
          except Exception:
              obj = None

          def collect(o):
              if isinstance(o, dict):
                  for k,v in o.items():
                      if k.lower() in {'store_id','id','storeid','number','code','store','storenumber','magasin'} and isinstance(v,(str,int)):
                          s = str(v).strip()
                          if s.isdigit():
                              ids.add(s.zfill(4))
                      else:
                          collect(v)
              elif isinstance(o, list):
                  for it in o:
                      collect(it)

          if obj is not None:
              collect(obj)

          # Sinon, regex brute (extrait nombres 3â€“5 chiffres, exclut annÃ©es)
          if not ids:
              for m in re.findall(r'\b\d{3,5}\b', raw):
                  if not (1900 <= int(m) <= 2099):
                      ids.add(m.zfill(4))

          ids = sorted(ids, key=lambda x: int(x))
          out.parent.mkdir(parents=True, exist_ok=True)
          out.write_text('\n'.join(ids), encoding='utf-8')
          print(f"[OK] Wrote {len(ids)} store IDs to {out}")
          PY

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies (Playwright, Requests, etc.)
        run: |
          python -m pip install --upgrade pip
          pip install playwright aiofiles pandas tqdm requests beautifulsoup4 lxml
          python -m playwright install-deps
          python -m playwright install

      # ðŸš€ Boucle sur TOUS les IDs (503) avec lâ€™interface de ton script (--store)
      - name: Run EconoDeal â€“ Canadian Tire scraper (all stores)
        env:
          CT_CONCURRENCY: "3"
        run: |
          set -e
          mkdir -p output_canadiantire
          rm -f output_canadiantire/canadiantire_liquidations_all_stores.csv || true

          echo "ðŸš€ Starting full scrapeâ€¦"
          TOTAL=$(wc -l < data/canadiantire/stores_ids.txt || echo 0)
          echo "Found $TOTAL store IDs."
          i=0
          while IFS= read -r STORE_ID; do
            [ -z "$STORE_ID" ] && continue
            i=$((i+1))
            echo "â–¶ï¸ ($i/$TOTAL) store $STORE_ID"
            python scrape_canadiantire_liquidations.py \
              --store "$STORE_ID" \
              --language fr \
              --output-dir output_canadiantire \
              --aggregated-path output_canadiantire/canadiantire_liquidations_all_stores.csv \
              --max-retries 3 \
              --timeout 30000 \
              --delay 1.6 3.0
          done < data/canadiantire/stores_ids.txt

          echo "âœ… Finished scraping all stores."

      - name: Upload scraping results
        uses: actions/upload-artifact@v4
        with:
          name: canadiantire-results
          path: output_canadiantire/**

      - name: Commit and push results to repository
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "actions@github.com"
          git add output_canadiantire || true
          git commit -m "ðŸ§© Canadian Tire 503-store scrape $(date -u +'%Y-%m-%d %H:%M:%S UTC')" || echo "No new data to commit"
          git push
